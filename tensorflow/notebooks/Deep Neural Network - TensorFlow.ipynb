{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cPickle as pickle\n",
    "import math\n",
    "import os\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from [Classifying MNIST digits using Logistic Regression](http://deeplearning.net/tutorial/logreg.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    dataset = \"mnist.pkl.gz\"\n",
    "    if ( not os.path.isfile(dataset) ):\n",
    "        print \"Download from {}\".format(\"http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz\")\n",
    "        return None\n",
    "    else:\n",
    "        print \"Loading data...\"\n",
    "\n",
    "        # Load the dataset\n",
    "        with gzip.open(dataset, 'rb') as f:\n",
    "            try:\n",
    "                train, valid, test = pickle.load(f, encoding='latin1')\n",
    "            except:\n",
    "                train, valid, test = pickle.load(f)\n",
    "                \n",
    "        ''' Reformat '''\n",
    "        train = ( train[0], reformat( train[1] ) )\n",
    "        test = ( test[0], reformat( test[1] ) )\n",
    "        valid = ( valid[0], reformat( valid[1] ) )\n",
    "        \n",
    "        print 'Training set', train[0].shape, train[1].shape\n",
    "        print 'Validation set', valid[0].shape, valid[1].shape\n",
    "        print 'Test set', test[0].shape, test[1].shape\n",
    "        return train, test, valid\n",
    "    \n",
    "def reformat( vec ):\n",
    "    ''' Convert vector into a one-hot vector '''\n",
    "    return ( np.arange( max( vec ) + 1 ) == vec[ :,None ] ).astype( np.float32 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Network implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DeepNeuralNetwork:\n",
    "    def __init__( self, n_in, n_out, test, valid, hidden_layers, activation=tf.nn.sigmoid, batch_size=128, learning_rate=0.01 ):\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "\n",
    "            ''' Training dataset, given in mini-batches '''\n",
    "            self.tf_train = ( tf.placeholder( tf.float32, shape=(batch_size, n_in ) ), tf.placeholder( tf.float32, shape=(batch_size, n_out ) ) )\n",
    "\n",
    "            ''' Validation dataset '''\n",
    "            tf_valid = ( tf.cast( tf.constant( valid[0] ), tf.float32 ), tf.cast( tf.constant( valid[1] ), tf.float32 ) )\n",
    "\n",
    "            ''' Testing dataset '''\n",
    "            tf_test = ( tf.cast( tf.constant( test[0] ), tf.float32 ), tf.cast( tf.constant( test[1] ), tf.float32 ) )\n",
    "\n",
    "            ''' Model '''\n",
    "            self.weights = [] #Weights list\n",
    "            self.bias = [] #Bias list\n",
    "\n",
    "            ''' L2 Regularization to avoid overfitting '''\n",
    "            self.l2_reg = 0.\n",
    "\n",
    "            '''Inputs'''\n",
    "            train_input = self.tf_train[0]\n",
    "            valid_input = tf_valid[0]\n",
    "            test_input = tf_test[0]\n",
    "\n",
    "            layer_in = n_in #number of incoming connections to the layer\n",
    "            ''' Add hidden layers '''\n",
    "            for layer_out, hdf in hidden_layers:\n",
    "                train_input = self._add_layer( train_input, layer_in, layer_out, activation=activation, dropout=hdf, l2_reg=True )\n",
    "                valid_input = self._add_layer( valid_input, layer_in, layer_out, activation=activation, weights=self.weights[-1], bias=self.bias[-1] )\n",
    "                test_input = self._add_layer( test_input, layer_in, layer_out, activation=activation, weights=self.weights[-1], bias=self.bias[-1] )\n",
    "                ''' Number of input connections to next layer is the number of output connections of the current layer '''\n",
    "                layer_in = layer_out\n",
    "                \n",
    "            ''' Output layers '''\n",
    "            train_logits = self._add_layer( train_input, layer_in, n_out )\n",
    "            valid_logits = self._add_layer( valid_input, layer_in, n_out, weights=self.weights[-1], bias=self.bias[-1] )\n",
    "            test_logits = self._add_layer( test_input, layer_in, n_out, weights=self.weights[-1], bias=self.bias[-1] )\n",
    "\n",
    "            ''' Cross-Entropy Cost function '''\n",
    "            self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(train_logits, self.tf_train[1])) + 0.0001 * self.l2_reg\n",
    "\n",
    "            ''' Adagrad '''\n",
    "            self.optimizer = tf.train.AdagradOptimizer( learning_rate ).minimize( self.cost )\n",
    "\n",
    "            ''' Prediction functions '''\n",
    "            self.train_pred = tf.nn.softmax( train_logits )\n",
    "            self.valid_pred = tf.nn.softmax( valid_logits )\n",
    "            self.test_pred = tf.nn.softmax( test_logits )\n",
    "\n",
    "    def _add_layer( self, input, n_in, n_out, activation=None, weights=None, bias=None, dropout=None, l2_reg=False ):\n",
    "        if( weights is None ):\n",
    "            ''' Xavier init '''\n",
    "            init_range = math.sqrt(6.0 / (n_in + n_out))\n",
    "            init_w = tf.random_uniform( [n_in,n_out], -init_range, init_range)\n",
    "            weights = tf.cast( tf.Variable( init_w ), tf.float32 )\n",
    "            self.weights.append( weights )\n",
    "\n",
    "        if( bias is None ):\n",
    "            bias = tf.cast( tf.Variable( tf.zeros( [ n_out ] ) ), tf.float32 )\n",
    "            self.bias.append( bias )\n",
    "\n",
    "        if( l2_reg ):\n",
    "            ''' L2 regularization '''\n",
    "            l2_reg = tf.nn.l2_loss( weights )\n",
    "            self.l2_reg += l2_reg\n",
    "\n",
    "        layer = tf.matmul( input, weights ) + bias\n",
    "        if( activation is not None ):\n",
    "            layer = activation( layer )\n",
    "\n",
    "        if( dropout is not None ):\n",
    "            ''' Dropout + scaling '''\n",
    "            layer = tf.nn.dropout( layer, 1-dropout ) * 1/( 1- dropout )\n",
    "\n",
    "        return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to compute accuracy given the prediction and the actual target values. `pred` is assumed to be in one-hot vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy( pred, labels ):\n",
    "    return ( 100.0 * np.sum( np.argmax( pred, 1 ) == np.argmax( labels, 1 ) ) / pred.shape[0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main function. Load dataset, create DNN model and perform training over minibatches. Validate the model every 100 epochs and finally test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Training set (50000, 784) (50000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n",
      "Cost at 0 - 3.41609025002\n",
      "Training accuracy : 9.375\n",
      "Validation accuracy : 24.96\n",
      "Cost at 100 - 0.543551087379\n",
      "Training accuracy : 84.375\n",
      "Validation accuracy : 89.68\n",
      "Cost at 200 - 0.432214826345\n",
      "Training accuracy : 88.28125\n",
      "Validation accuracy : 91.99\n",
      "Cost at 300 - 0.360873788595\n",
      "Training accuracy : 93.75\n",
      "Validation accuracy : 92.73\n",
      "Cost at 400 - 0.447462260723\n",
      "Training accuracy : 88.28125\n",
      "Validation accuracy : 93.4\n",
      "Cost at 500 - 0.427373856306\n",
      "Training accuracy : 86.71875\n",
      "Validation accuracy : 94.07\n",
      "Cost at 600 - 0.277434289455\n",
      "Training accuracy : 93.75\n",
      "Validation accuracy : 94.51\n",
      "Cost at 700 - 0.335170388222\n",
      "Training accuracy : 93.75\n",
      "Validation accuracy : 94.65\n",
      "Cost at 800 - 0.401612192392\n",
      "Training accuracy : 92.1875\n",
      "Validation accuracy : 94.75\n",
      "Cost at 900 - 0.25052523613\n",
      "Training accuracy : 96.09375\n",
      "Validation accuracy : 95.25\n",
      "Cost at 1000 - 0.380655646324\n",
      "Training accuracy : 92.1875\n",
      "Validation accuracy : 95.33\n",
      "Cost at 1100 - 0.264751315117\n",
      "Training accuracy : 93.75\n",
      "Validation accuracy : 95.77\n",
      "Cost at 1200 - 0.263192802668\n",
      "Training accuracy : 95.3125\n",
      "Validation accuracy : 95.8\n",
      "Cost at 1300 - 0.262911438942\n",
      "Training accuracy : 95.3125\n",
      "Validation accuracy : 95.8\n",
      "Cost at 1400 - 0.250732451677\n",
      "Training accuracy : 94.53125\n",
      "Validation accuracy : 95.89\n",
      "Cost at 1500 - 0.44625556469\n",
      "Training accuracy : 89.84375\n",
      "Validation accuracy : 96.23\n",
      "Cost at 1600 - 0.258660942316\n",
      "Training accuracy : 94.53125\n",
      "Validation accuracy : 96.24\n",
      "Cost at 1700 - 0.197013258934\n",
      "Training accuracy : 96.875\n",
      "Validation accuracy : 96.15\n",
      "Cost at 1800 - 0.2762350142\n",
      "Training accuracy : 96.09375\n",
      "Validation accuracy : 96.36\n",
      "Cost at 1900 - 0.223351657391\n",
      "Training accuracy : 95.3125\n",
      "Validation accuracy : 96.48\n",
      "Cost at 2000 - 0.223473533988\n",
      "Training accuracy : 96.09375\n",
      "Validation accuracy : 96.51\n",
      "Cost at 2100 - 0.206025615335\n",
      "Training accuracy : 97.65625\n",
      "Validation accuracy : 96.64\n",
      "Cost at 2200 - 0.254069328308\n",
      "Training accuracy : 94.53125\n",
      "Validation accuracy : 96.72\n",
      "Cost at 2300 - 0.272744715214\n",
      "Training accuracy : 92.1875\n",
      "Validation accuracy : 96.7\n",
      "Cost at 2400 - 0.203752085567\n",
      "Training accuracy : 95.3125\n",
      "Validation accuracy : 96.72\n",
      "Cost at 2500 - 0.280999422073\n",
      "Training accuracy : 95.3125\n",
      "Validation accuracy : 96.65\n",
      "Cost at 2600 - 0.249020859599\n",
      "Training accuracy : 96.09375\n",
      "Validation accuracy : 96.79\n",
      "Cost at 2700 - 0.272262334824\n",
      "Training accuracy : 93.75\n",
      "Validation accuracy : 96.92\n",
      "Cost at 2800 - 0.20736220479\n",
      "Training accuracy : 97.65625\n",
      "Validation accuracy : 97.0\n",
      "Cost at 2900 - 0.289493560791\n",
      "Training accuracy : 92.96875\n",
      "Validation accuracy : 97.09\n",
      "Cost at 3000 - 0.198731049895\n",
      "Training accuracy : 95.3125\n",
      "Validation accuracy : 97.05\n",
      "Cost at 3100 - 0.195785284042\n",
      "Training accuracy : 96.875\n",
      "Validation accuracy : 97.12\n",
      "Cost at 3200 - 0.192126512527\n",
      "Training accuracy : 96.875\n",
      "Validation accuracy : 97.03\n",
      "Cost at 3300 - 0.147364109755\n",
      "Training accuracy : 99.21875\n",
      "Validation accuracy : 97.1\n",
      "Cost at 3400 - 0.170010700822\n",
      "Training accuracy : 98.4375\n",
      "Validation accuracy : 97.15\n",
      "Cost at 3500 - 0.19149518013\n",
      "Training accuracy : 95.3125\n",
      "Validation accuracy : 97.23\n",
      "Cost at 3600 - 0.160430192947\n",
      "Training accuracy : 98.4375\n",
      "Validation accuracy : 97.27\n",
      "Cost at 3700 - 0.22791031003\n",
      "Training accuracy : 98.4375\n",
      "Validation accuracy : 97.36\n",
      "Cost at 3800 - 0.213408157229\n",
      "Training accuracy : 96.09375\n",
      "Validation accuracy : 97.39\n",
      "Cost at 3900 - 0.271446734667\n",
      "Training accuracy : 96.875\n",
      "Validation accuracy : 97.28\n",
      "Cost at 4000 - 0.195589050651\n",
      "Training accuracy : 95.3125\n",
      "Validation accuracy : 97.4\n",
      "Cost at 4100 - 0.150971457362\n",
      "Training accuracy : 98.4375\n",
      "Validation accuracy : 97.37\n",
      "Cost at 4200 - 0.15701828897\n",
      "Training accuracy : 97.65625\n",
      "Validation accuracy : 97.46\n",
      "Cost at 4300 - 0.14725536108\n",
      "Training accuracy : 98.4375\n",
      "Validation accuracy : 97.45\n",
      "Cost at 4400 - 0.20180028677\n",
      "Training accuracy : 96.875\n",
      "Validation accuracy : 97.45\n",
      "Cost at 4500 - 0.227844536304\n",
      "Training accuracy : 93.75\n",
      "Validation accuracy : 97.42\n",
      "Cost at 4600 - 0.212880253792\n",
      "Training accuracy : 94.53125\n",
      "Validation accuracy : 97.6\n",
      "Cost at 4700 - 0.164814889431\n",
      "Training accuracy : 98.4375\n",
      "Validation accuracy : 97.51\n",
      "Cost at 4800 - 0.251235038042\n",
      "Training accuracy : 95.3125\n",
      "Validation accuracy : 97.56\n",
      "Cost at 4900 - 0.147378802299\n",
      "Training accuracy : 97.65625\n",
      "Validation accuracy : 97.64\n",
      "Test accuracy : 97.63\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    ''' Dataset '''\n",
    "    train,valid,test = load_data()\n",
    "    train_X = train[0]\n",
    "    train_Y = train[1]\n",
    "\n",
    "    ''' Params '''\n",
    "    n_epochs = 5000 #Number of epochs\n",
    "    batch_size = 128 #Batch size\n",
    "    learning_rate = 0.01 #Learning rate\n",
    "    hidden_layers = [ ( 1024, 0.5 ), ( 1024, 0.5 ) ] #Number of hidden neurons and corresponding dropout factor\n",
    "    n_in = train[0].shape[1] #Number of input neurons\n",
    "    n_out = train[1].shape[1] #Number of ouptut neurons - number of classes\n",
    "\n",
    "    ''' Model '''\n",
    "    dnn = DeepNeuralNetwork( n_in, n_out, test, valid, hidden_layers, tf.nn.relu, batch_size, learning_rate )\n",
    "\n",
    "    with tf.Session( graph = dnn.graph ) as session:\n",
    "        ''' Initialize TensorFlow variables '''\n",
    "        tf.initialize_all_variables().run()\n",
    "        for ep in range( n_epochs ):\n",
    "            ''' Mini-batching '''\n",
    "            offset = (ep * batch_size) % (train_Y.shape[0] - batch_size)\n",
    "            batch_X = train_X[ offset:(offset + batch_size) ]\n",
    "            batch_Y = train_Y[ offset:(offset + batch_size) ]\n",
    "\n",
    "            ''' Input to placeholders '''\n",
    "            feed_dict = { dnn.tf_train[0]:batch_X, dnn.tf_train[1]:batch_Y }\n",
    "\n",
    "            ''' Train step '''\n",
    "            _, cost, train_pred = session.run( [ dnn.optimizer, dnn.cost, dnn.train_pred ], feed_dict=feed_dict )\n",
    "\n",
    "            if( ep % 100 == 0 ):\n",
    "                print \"Cost at {} - {}\".format( ep, cost )\n",
    "                print \"Training accuracy : {}\".format( accuracy( train_pred, batch_Y ) )\n",
    "                print \"Validation accuracy : {}\".format( accuracy( dnn.valid_pred.eval(), valid[1] ) )\n",
    "\n",
    "        ''' Testing '''\n",
    "        print \"Test accuracy : {}\".format( accuracy( dnn.test_pred.eval(), test[1] ) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
